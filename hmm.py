## Helper functions for building the Hidden Markov Model 

import numpy as np 
from sklearn.cluster import KMeans
from scipy.stats import multivariate_normal

def activity_sequence(i, activityIndex, x, K):
    """ Returns a (K,561) array, where each row is the average of a certain number
    of frames."""
    
    # i is the index of the activity sequence (400 instances in total)
    # activityIndex is the matrix generated by initialize_hmm.segment() function 
    # x is the associated data set (training or testing)
    # K is the number of states per activity sequence (7 in the literature)
    
    # Returns a (7,561) array, where each row is the average of a certain number 
    # of frames. The number of frames averaged is in the variable frames_per_state.
    
    # get the specific frames, and the length of the activity
    start_ind  = activityIndex[i, 1]
    end_ind    = activityIndex[i, 2]
    length     = end_ind - start_ind
    frame_data = x[int(start_ind):int(end_ind), :]
    
    # need to determine how many frames are in each state
    if length >= K:
        frames_per_state = int(length//K)
        #print('~{} frames per state.'.format(frames_per_state))
        # segment the frame data (average the vectors?)
        states = []
        index = 0
        for j in range(0, K):
            data = np.mean(frame_data[index:index + frames_per_state,:], axis = 0)
            states.append(data)
            index += frames_per_state
        states = np.asarray(states)
        valid = True
    else:
        states = []
        valid = False
    return states, valid

def cal_b(x,miu,covar):
  """ This function calculates the emission probability given the 
  means and covariance matrix for a multivariate Gaussian """

  # check if we are using only the diagnal elements of the cov matrix
  if covar.ndim==1:
    pdf = multivariate_normal.pdf(x, mean=miu, cov=np.diag(covar))
    return pdf 

def cal_b_matrix(x, miu, covar, H, K):
    """ This function calculates the multivariate pdf for a sequence of data for 
    both hidden states assuming the covariance has a diagonal form. This simplifies 
    the multivariate Gaussian to just a product of univariate Gaussians. """
    B = np.zeros((K,H))
    for h in range(H):
        for k in range(K):
            tmp = cal_b(x[k,:], miu[h,:], covar[h,:])
            if np.isinf(tmp):
                tmp = 1e307 # an upper bound is applied to improve stability
            elif tmp == 0:
                tmp = 1e-307 # a lower bound is applied to improve stability 
            B[k,h] = tmp
    return B

def cal_b_matrix_GMM(x, miu, covar, w, H, K):
    """ This function calculates the pdf for a Gaussian mixture for all x for all 
    hidden states and Gaussian mixtures. """
    
    # weights should be of dimension (H, n_mixture)
    # returns a matrix B that is (T, H, n_mixture)
    # H is hidden states
    # K is length of sequence (7) 
    
    M = w.shape[1] # number of mixtures
    B = np.zeros((K,H,M))
    for h in range(H):
        for k in range(K):
            for m in range(M):
                tmp = cal_b(x[k,:], miu[h,:,m], covar[h,:,m])

                if np.isinf(tmp):
                    tmp = 1e30
                elif tmp == 0:
                    tmp = 1e-300
                    
                B[k,h,m] = tmp
    return B

def all_sequences(x_train, L, segments):
    # Run activity_sequence on all the datas of the same label
    # segments is the output of initialize_hmm.segment_data()
    # Returns a list of (7,561) dimensional arrays 
    data_sequence = []
    for i in range(len(segments)):
        if segments[i,0] == L:
            states, valid = activity_sequence(i, segments, x_train, 7)
            if valid: 
                data_sequence.append(states)
    return data_sequence

def initialize_Gaussians(x, n_Gauss):
  # Use k-means on the input data to determine the initial centers and variances of the Gaussians
  # perform k-means on the entire data set, with number of clusters equal to number of hidden states 
  
  # parse the segmented x_train data into a matrix
  x= np.concatenate((x), axis = 0)
  
  main_kmeans = KMeans(n_clusters = n_Gauss, random_state = 1).fit(x)  
  Gauss_means = main_kmeans.cluster_centers_
  
  n_feature = x.shape[1] # 561 if we are using all features
  # compute covariance using clustered samples (diagonal matrix, n_components by n_features )
  labels = main_kmeans.labels_
  covar  = np.zeros((n_Gauss, n_feature))
  for i in range(n_Gauss):
    x_clus = x[np.where(labels==i)]
    x_covar  = np.var(x_clus.T, axis = 1)
    
    # set lower bound in the Gaussians
    for j in range(n_feature):
        if x_covar[j] < 1e-3:
            x_covar[j] = 1e-3
            
    covar[i,:] = x_covar
  return main_kmeans, Gauss_means, covar

def initialize_GaussianMixture(x, n_Gauss, n_mixture):
  # Use k-means on the input data to determine the initial centers and variances of the Gaussian mixtures
  # Use k-means again on the subsets to initialize the mixture components 
  
  # parse the segmented x_train data into a matrix
  x= np.concatenate((x), axis = 0)
  
  unbalanced = 1
  seed = 0
  while unbalanced:
      print("\nK-means iteration")
      main_kmeans = KMeans(n_clusters = n_Gauss, random_state = seed).fit(x)
      
      for i in range(n_Gauss):
          n_samples = np.sum(main_kmeans.labels_ == i)
          print("Cluster {}: {} samples".format(i,n_samples))
          if n_samples < n_mixture:
              unbalanced = 1
              seed += 1
              print("seed = {}".format(seed))
              break
          else:
              unbalanced = 0
      print("Unbalanced: False".format(unbalanced))
  labels = main_kmeans.labels_
    
  kmeanses = []
  for label in range(n_Gauss):
      kmeans = KMeans(n_clusters = n_mixture, random_state = 0)
      kmeans.fit(x[np.where(labels == label)])
      kmeanses.append(kmeans)
  
  # compute covariance using clustered samples 
  # for diagonal matrix, dimensions are (n_components, n_features, n_mixture)
  # the dimension of the means are (n_components, n_features, n_mixture)
  n_feature = x.shape[1] # 561 if we are using all features
  var  = np.zeros((n_Gauss, n_feature, n_mixture))
  means = np.zeros((n_Gauss, n_feature, n_mixture))
  
  for i in range(n_Gauss):
    for j in range(n_mixture):
        means[i,:,j] = (kmeanses[i]).cluster_centers_[j,:]
        x_clus = x[np.where(kmeanses[i].labels_==j)]
        x_covar  = np.var(x_clus.T, axis = 1)
        
        # set lower bound in the Gaussians
        for q in range(n_feature):
            if x_covar[q] < 1e-3:
                x_covar[q] = 1e-3
        var[i,:,j] = x_covar   
    
  return kmeanses, means, var












